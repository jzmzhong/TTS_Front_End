Loading GBERT from: ../../checkpoints/2_GBERT/GBERT_EnUs_layer6_dim512_ffn4_head8/model_step_935k.pt
AutoregressiveTransformer(
  (encoder_embedding): Embedding(32, 512)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder_embedding): Embedding(72, 512)
  (pos_decoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (fc_out): Linear(in_features=512, out_features=72, bias=True)
)
CrossEntropyLoss(
  (criterion): CrossEntropyLoss()
)
Training on device: cuda
Pretrained Layers that have lower LR: ['encoder_embedding.weight', 'pos_encoder.scale', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias']
Newly Randomized Layers that have base LR: ['decoder_embedding.weight', 'pos_decoder.scale', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc_out.weight', 'fc_out.bias']
Epoch: 1 | Step 156 | Train Loss: 4.447
Epoch: 2 | Step 312 | Train Loss: 3.823
Epoch: 3 | Step 468 | Train Loss: 3.461
Epoch: 4 | Step 624 | Train Loss: 3.310
Epoch: 5 | Step 780 | Train Loss: 3.212
Epoch: 6 | Step 936 | Train Loss: 3.108
Epoch: 7 | Step 1092 | Train Loss: 3.027
Epoch: 8 | Step 1248 | Train Loss: 2.895
Epoch: 9 | Step 1404 | Train Loss: 2.779
Epoch: 10 | Step 1560 | Train Loss: 2.637
Epoch: 11 | Step 1716 | Train Loss: 2.524
Epoch: 12 | Step 1872 | Train Loss: 2.398
Epoch: 13 | Step 2000 | Valid Loss: 2.303
Epoch: 13 | Step 2000 | Mean PER: 0.7432127461181908 | Mean WER: 0.9977021276595744
Loading GBERT from: ../../checkpoints/2_GBERT/GBERT_EnUs_layer6_dim512_ffn4_head8/model_step_935k.pt
AutoregressiveTransformer(
  (encoder_embedding): Embedding(32, 512)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder_embedding): Embedding(72, 512)
  (pos_decoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (fc_out): Linear(in_features=512, out_features=72, bias=True)
)
CrossEntropyLoss(
  (criterion): CrossEntropyLoss()
)
Training on device: cuda
Pretrained Layers that have lower LR: ['encoder_embedding.weight', 'pos_encoder.scale', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias']
Newly Randomized Layers that have base LR: ['decoder_embedding.weight', 'pos_decoder.scale', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc_out.weight', 'fc_out.bias']
Epoch: 1 | Step 156 | Train Loss: 4.240
Epoch: 2 | Step 312 | Train Loss: 3.718
Epoch: 3 | Step 468 | Train Loss: 3.437
Epoch: 4 | Step 624 | Train Loss: 3.307
Epoch: 5 | Step 780 | Train Loss: 3.216
Epoch: 6 | Step 936 | Train Loss: 3.118
Epoch: 7 | Step 1092 | Train Loss: 3.045
Epoch: 8 | Step 1248 | Train Loss: 2.915
Epoch: 9 | Step 1404 | Train Loss: 2.799
Epoch: 10 | Step 1560 | Train Loss: 2.663
Epoch: 11 | Step 1716 | Train Loss: 2.542
Epoch: 12 | Step 1872 | Train Loss: 2.418
Epoch: 13 | Step 2000 | Valid Loss: 2.321
Epoch: 13 | Step 2000 | Mean PER: 0.7536182921390829 | Mean WER: 0.9970212765957447
Achieving better result (mean_per): 0.7536
Epoch: 13 | Step 2028 | Train Loss: 2.304
Epoch: 14 | Step 2184 | Train Loss: 2.189
Epoch: 15 | Step 2340 | Train Loss: 2.055
Epoch: 16 | Step 2496 | Train Loss: 1.944
Epoch: 17 | Step 2652 | Train Loss: 1.802
Epoch: 18 | Step 2808 | Train Loss: 1.694
Epoch: 19 | Step 2964 | Train Loss: 1.597
Epoch: 20 | Step 3120 | Train Loss: 1.520
Epoch: 21 | Step 3276 | Train Loss: 1.424
Epoch: 22 | Step 3432 | Train Loss: 1.347
Epoch: 23 | Step 3588 | Train Loss: 1.275
Epoch: 24 | Step 3744 | Train Loss: 1.198
Epoch: 25 | Step 3900 | Train Loss: 1.140
Epoch: 26 | Step 4000 | Valid Loss: 1.100
Epoch: 26 | Step 4000 | Mean PER: 0.4126271976648333 | Mean WER: 0.8539574468085106
Achieving better result (mean_per): 0.4126
Epoch: 26 | Step 4056 | Train Loss: 1.093
Epoch: 27 | Step 4212 | Train Loss: 1.032
Epoch: 28 | Step 4368 | Train Loss: 0.9800
Epoch: 29 | Step 4524 | Train Loss: 0.9327
Epoch: 30 | Step 4680 | Train Loss: 0.8743
Epoch: 31 | Step 4836 | Train Loss: 0.8319
Epoch: 32 | Step 4992 | Train Loss: 0.7935
Epoch: 33 | Step 5148 | Train Loss: 0.7575
Epoch: 34 | Step 5304 | Train Loss: 0.7187
Epoch: 35 | Step 5460 | Train Loss: 0.6941
Epoch: 36 | Step 5616 | Train Loss: 0.6574
Epoch: 37 | Step 5772 | Train Loss: 0.6441
Epoch: 38 | Step 5928 | Train Loss: 0.6084
Epoch: 39 | Step 6000 | Valid Loss: 0.6024
Epoch: 39 | Step 6000 | Mean PER: 0.2671522588143083 | Mean WER: 0.6877446808510639
Achieving better result (mean_per): 0.2672
Epoch: 39 | Step 6084 | Train Loss: 0.6047
Epoch: 40 | Step 6240 | Train Loss: 0.5660
Epoch: 41 | Step 6396 | Train Loss: 0.5398
Epoch: 42 | Step 6552 | Train Loss: 0.5186
Epoch: 43 | Step 6708 | Train Loss: 0.5002
Epoch: 44 | Step 6864 | Train Loss: 0.4874
Epoch: 45 | Step 7020 | Train Loss: 0.4633
Epoch: 46 | Step 7176 | Train Loss: 0.4488
Epoch: 47 | Step 7332 | Train Loss: 0.4280
Epoch: 48 | Step 7488 | Train Loss: 0.4187
Epoch: 49 | Step 7644 | Train Loss: 0.4063
Epoch: 50 | Step 7800 | Train Loss: 0.3887
Epoch: 51 | Step 7956 | Train Loss: 0.3792
Epoch: 52 | Step 8000 | Valid Loss: 0.3439
Epoch: 52 | Step 8000 | Mean PER: 0.20373248287139015 | Mean WER: 0.5785531914893617
Achieving better result (mean_per): 0.2037
Epoch: 52 | Step 8112 | Train Loss: 0.3678
Epoch: 53 | Step 8268 | Train Loss: 0.3507
Epoch: 54 | Step 8424 | Train Loss: 0.3439
Epoch: 55 | Step 8580 | Train Loss: 0.3292
Epoch: 56 | Step 8736 | Train Loss: 0.3244
Epoch: 57 | Step 8892 | Train Loss: 0.3124
Epoch: 58 | Step 9048 | Train Loss: 0.3026
Epoch: 59 | Step 9204 | Train Loss: 0.2927
Epoch: 60 | Step 9360 | Train Loss: 0.2851
Epoch: 61 | Step 9516 | Train Loss: 0.2752
Epoch: 62 | Step 9672 | Train Loss: 0.2663
Epoch: 63 | Step 9828 | Train Loss: 0.2606
Epoch: 64 | Step 9984 | Train Loss: 0.2510
Epoch: 65 | Step 10000 | Valid Loss: 0.2919
Epoch: 65 | Step 10000 | Mean PER: 0.1743807348747956 | Mean WER: 0.5256170212765957
Achieving better result (mean_per): 0.1744
Epoch: 65 | Step 10140 | Train Loss: 0.2459
Epoch: 66 | Step 10296 | Train Loss: 0.2395
Epoch: 67 | Step 10452 | Train Loss: 0.2256
Epoch: 68 | Step 10608 | Train Loss: 0.2183
Epoch: 69 | Step 10764 | Train Loss: 0.2161
Epoch: 70 | Step 10920 | Train Loss: 0.2072
Epoch: 71 | Step 11076 | Train Loss: 0.1985
Epoch: 72 | Step 11232 | Train Loss: 0.1933
Epoch: 73 | Step 11388 | Train Loss: 0.1853
Epoch: 74 | Step 11544 | Train Loss: 0.1797
Epoch: 75 | Step 11700 | Train Loss: 0.1781
Epoch: 76 | Step 11856 | Train Loss: 0.1703
Epoch: 77 | Step 12000 | Valid Loss: 0.1721
Epoch: 77 | Step 12000 | Mean PER: 0.1653265584670063 | Mean WER: 0.4980425531914894
Achieving better result (mean_per): 0.1653
Epoch: 77 | Step 12012 | Train Loss: 0.1689
Epoch: 78 | Step 12168 | Train Loss: 0.1612
Epoch: 79 | Step 12324 | Train Loss: 0.1576
Epoch: 80 | Step 12480 | Train Loss: 0.1506
Epoch: 81 | Step 12636 | Train Loss: 0.1458
Epoch: 82 | Step 12792 | Train Loss: 0.1392
Epoch: 83 | Step 12948 | Train Loss: 0.1370
Epoch: 84 | Step 13104 | Train Loss: 0.1330
Epoch: 85 | Step 13260 | Train Loss: 0.1294
Epoch: 86 | Step 13416 | Train Loss: 0.1267
Epoch: 87 | Step 13572 | Train Loss: 0.1225
Epoch: 88 | Step 13728 | Train Loss: 0.1212
Epoch: 89 | Step 13884 | Train Loss: 0.1163
Epoch: 90 | Step 14000 | Valid Loss: 0.1202
Epoch: 90 | Step 14000 | Mean PER: 0.15698860795416153 | Mean WER: 0.49259574468085104
Achieving better result (mean_per): 0.1570
Epoch: 90 | Step 14040 | Train Loss: 0.1134
Epoch: 91 | Step 14196 | Train Loss: 0.1096
Epoch: 92 | Step 14352 | Train Loss: 0.1058
Epoch: 93 | Step 14508 | Train Loss: 0.1022
Epoch: 94 | Step 14664 | Train Loss: 0.1012
Epoch: 95 | Step 14820 | Train Loss: 0.09780
Epoch: 96 | Step 14976 | Train Loss: 0.09805
Epoch: 97 | Step 15132 | Train Loss: 0.09340
Epoch: 98 | Step 15288 | Train Loss: 0.09083
Epoch: 99 | Step 15444 | Train Loss: 0.09083
Epoch: 100 | Step 15600 | Train Loss: 0.08665
Epoch: 101 | Step 15756 | Train Loss: 0.08186
Epoch: 102 | Step 15912 | Train Loss: 0.08161
Epoch: 103 | Step 16000 | Valid Loss: 0.07215
Epoch: 103 | Step 16000 | Mean PER: 0.15252908823092204 | Mean WER: 0.48561702127659573
Achieving better result (mean_per): 0.1525
Epoch: 103 | Step 16068 | Train Loss: 0.07819
Epoch: 104 | Step 16224 | Train Loss: 0.07891
Epoch: 105 | Step 16380 | Train Loss: 0.07441
Epoch: 106 | Step 16536 | Train Loss: 0.07460
Epoch: 107 | Step 16692 | Train Loss: 0.07105
Epoch: 108 | Step 16848 | Train Loss: 0.07211
Epoch: 109 | Step 17004 | Train Loss: 0.06936
Epoch: 110 | Step 17160 | Train Loss: 0.06768
Epoch: 111 | Step 17316 | Train Loss: 0.06546
Epoch: 112 | Step 17472 | Train Loss: 0.06305
Epoch: 113 | Step 17628 | Train Loss: 0.06242
Epoch: 114 | Step 17784 | Train Loss: 0.06194
Epoch: 115 | Step 17940 | Train Loss: 0.06061
Epoch: 116 | Step 18000 | Valid Loss: 0.06028
Epoch: 116 | Step 18000 | Mean PER: 0.15162367059014312 | Mean WER: 0.4826382978723404
Achieving better result (mean_per): 0.1516
Epoch: 116 | Step 18096 | Train Loss: 0.05997
Epoch: 117 | Step 18252 | Train Loss: 0.05775
Epoch: 118 | Step 18408 | Train Loss: 0.05477
Epoch: 119 | Step 18564 | Train Loss: 0.05536
Epoch: 120 | Step 18720 | Train Loss: 0.05589
Epoch: 121 | Step 18876 | Train Loss: 0.05268
Epoch: 122 | Step 19032 | Train Loss: 0.05176
Epoch: 123 | Step 19188 | Train Loss: 0.05221
Epoch: 124 | Step 19344 | Train Loss: 0.05005
Epoch: 125 | Step 19500 | Train Loss: 0.04864
Epoch: 126 | Step 19656 | Train Loss: 0.04807
Epoch: 127 | Step 19812 | Train Loss: 0.04652
Epoch: 128 | Step 19968 | Train Loss: 0.04729
Epoch: 129 | Step 20000 | Valid Loss: 0.04618
Epoch: 129 | Step 20000 | Mean PER: 0.14886687657941325 | Mean WER: 0.4753191489361702
Achieving better result (mean_per): 0.1489
Epoch: 129 | Step 20124 | Train Loss: 0.04551
Epoch: 130 | Step 20280 | Train Loss: 0.04464
Epoch: 131 | Step 20436 | Train Loss: 0.04357
Epoch: 132 | Step 20592 | Train Loss: 0.04233
Epoch: 133 | Step 20748 | Train Loss: 0.04227
Epoch: 134 | Step 20904 | Train Loss: 0.04070
Epoch: 135 | Step 21060 | Train Loss: 0.03925
Epoch: 136 | Step 21216 | Train Loss: 0.03999
Epoch: 137 | Step 21372 | Train Loss: 0.03972
Epoch: 138 | Step 21528 | Train Loss: 0.04040
Epoch: 139 | Step 21684 | Train Loss: 0.03719
Epoch: 140 | Step 21840 | Train Loss: 0.03661
Epoch: 141 | Step 21996 | Train Loss: 0.03684
Epoch: 142 | Step 22000 | Valid Loss: 0.05137
Epoch: 142 | Step 22000 | Mean PER: 0.14944796551304748 | Mean WER: 0.4745531914893617
Epoch: 142 | Step 22152 | Train Loss: 0.03515
Epoch: 143 | Step 22308 | Train Loss: 0.03603
Epoch: 144 | Step 22464 | Train Loss: 0.03414
Epoch: 145 | Step 22620 | Train Loss: 0.03375
Epoch: 146 | Step 22776 | Train Loss: 0.03329
Epoch: 147 | Step 22932 | Train Loss: 0.03430
Epoch: 148 | Step 23088 | Train Loss: 0.03296
Epoch: 149 | Step 23244 | Train Loss: 0.03161
Epoch: 150 | Step 23400 | Train Loss: 0.03026
Epoch: 151 | Step 23556 | Train Loss: 0.03032
Epoch: 152 | Step 23712 | Train Loss: 0.02918
Epoch: 153 | Step 23868 | Train Loss: 0.02902
Epoch: 154 | Step 24000 | Valid Loss: 0.02927
Epoch: 154 | Step 24000 | Mean PER: 0.14690739064041405 | Mean WER: 0.4716595744680851
Achieving better result (mean_per): 0.1469
Epoch: 154 | Step 24024 | Train Loss: 0.02949
Epoch: 155 | Step 24180 | Train Loss: 0.02833
Epoch: 156 | Step 24336 | Train Loss: 0.02768
Epoch: 157 | Step 24492 | Train Loss: 0.02758
Epoch: 158 | Step 24648 | Train Loss: 0.02715
Epoch: 159 | Step 24804 | Train Loss: 0.02657
Epoch: 160 | Step 24960 | Train Loss: 0.02603
Epoch: 161 | Step 25116 | Train Loss: 0.02562
Epoch: 162 | Step 25272 | Train Loss: 0.02662
Epoch: 163 | Step 25428 | Train Loss: 0.02717
Epoch: 164 | Step 25584 | Train Loss: 0.02559
Epoch: 165 | Step 25740 | Train Loss: 0.02427
Epoch: 166 | Step 25896 | Train Loss: 0.02445
Epoch: 167 | Step 26000 | Valid Loss: 0.02353
Epoch: 167 | Step 26000 | Mean PER: 0.1472182056514277 | Mean WER: 0.47106382978723405
Epoch: 167 | Step 26052 | Train Loss: 0.02394
Epoch: 168 | Step 26208 | Train Loss: 0.02562
Epoch: 169 | Step 26364 | Train Loss: 0.02445
Epoch: 170 | Step 26520 | Train Loss: 0.02371
Epoch: 171 | Step 26676 | Train Loss: 0.02326
Epoch: 172 | Step 26832 | Train Loss: 0.02222
Epoch: 173 | Step 26988 | Train Loss: 0.02298
Epoch: 174 | Step 27144 | Train Loss: 0.02286
Epoch: 175 | Step 27300 | Train Loss: 0.02089
Epoch: 176 | Step 27456 | Train Loss: 0.02327
Epoch: 177 | Step 27612 | Train Loss: 0.02299
Epoch: 178 | Step 27768 | Train Loss: 0.02159
Epoch: 179 | Step 27924 | Train Loss: 0.02254
Epoch: 180 | Step 28000 | Valid Loss: 0.02040
Epoch: 180 | Step 28000 | Mean PER: 0.1479074041541102 | Mean WER: 0.4733617021276596
Epoch: 180 | Step 28080 | Train Loss: 0.02071
Epoch: 181 | Step 28236 | Train Loss: 0.02042
Epoch: 182 | Step 28392 | Train Loss: 0.02003
Epoch: 183 | Step 28548 | Train Loss: 0.02105
Epoch: 184 | Step 28704 | Train Loss: 0.02134
Epoch: 185 | Step 28860 | Train Loss: 0.01981
Epoch: 186 | Step 29016 | Train Loss: 0.01791
Epoch: 187 | Step 29172 | Train Loss: 0.02040
Epoch: 188 | Step 29328 | Train Loss: 0.01962
Epoch: 189 | Step 29484 | Train Loss: 0.01779
Epoch: 190 | Step 29640 | Train Loss: 0.01796
Epoch: 191 | Step 29796 | Train Loss: 0.01929
Epoch: 192 | Step 29952 | Train Loss: 0.02045
Epoch: 193 | Step 30000 | Valid Loss: 0.01670
Epoch: 193 | Step 30000 | Mean PER: 0.14562358950796633 | Mean WER: 0.468
Achieving better result (mean_per): 0.1456
Epoch: 193 | Step 30108 | Train Loss: 0.01889
Epoch: 194 | Step 30264 | Train Loss: 0.01713
Epoch: 195 | Step 30420 | Train Loss: 0.01701
Epoch: 196 | Step 30576 | Train Loss: 0.01553
Epoch: 197 | Step 30732 | Train Loss: 0.01776
Epoch: 198 | Step 30888 | Train Loss: 0.01725
Epoch: 199 | Step 31044 | Train Loss: 0.01686
Epoch: 200 | Step 31200 | Train Loss: 0.01721
Epoch: 201 | Step 31356 | Train Loss: 0.01632
Epoch: 202 | Step 31512 | Train Loss: 0.01772
Epoch: 203 | Step 31668 | Train Loss: 0.01534
Epoch: 204 | Step 31824 | Train Loss: 0.01590
Epoch: 205 | Step 31980 | Train Loss: 0.01701
Epoch: 206 | Step 32000 | Valid Loss: 0.01555
Epoch: 206 | Step 32000 | Mean PER: 0.14397491857998082 | Mean WER: 0.4687659574468085
Achieving better result (mean_per): 0.1440
Epoch: 206 | Step 32136 | Train Loss: 0.01633
Epoch: 207 | Step 32292 | Train Loss: 0.01643
Epoch: 208 | Step 32448 | Train Loss: 0.01557
Epoch: 209 | Step 32604 | Train Loss: 0.01650
Epoch: 210 | Step 32760 | Train Loss: 0.01548
Epoch: 211 | Step 32916 | Train Loss: 0.01506
Epoch: 212 | Step 33072 | Train Loss: 0.01579
Epoch: 213 | Step 33228 | Train Loss: 0.01508
Epoch: 214 | Step 33384 | Train Loss: 0.01479
Epoch: 215 | Step 33540 | Train Loss: 0.01420
Epoch: 216 | Step 33696 | Train Loss: 0.01540
Epoch: 217 | Step 33852 | Train Loss: 0.01324
Epoch: 218 | Step 34000 | Valid Loss: 0.01396
Epoch: 218 | Step 34000 | Mean PER: 0.14313706941985702 | Mean WER: 0.466468085106383
Achieving better result (mean_per): 0.1431
Epoch: 218 | Step 34008 | Train Loss: 0.01427
Epoch: 219 | Step 34164 | Train Loss: 0.01426
Epoch: 220 | Step 34320 | Train Loss: 0.01386
Epoch: 221 | Step 34476 | Train Loss: 0.01376
Epoch: 222 | Step 34632 | Train Loss: 0.01424
Epoch: 223 | Step 34788 | Train Loss: 0.01408
Epoch: 224 | Step 34944 | Train Loss: 0.01235
Epoch: 225 | Step 35100 | Train Loss: 0.01343
Epoch: 226 | Step 35256 | Train Loss: 0.01490
Epoch: 227 | Step 35412 | Train Loss: 0.01251
Epoch: 228 | Step 35568 | Train Loss: 0.01370
Epoch: 229 | Step 35724 | Train Loss: 0.01463
Epoch: 230 | Step 35880 | Train Loss: 0.01332
Epoch: 231 | Step 36000 | Valid Loss: 0.01307
Epoch: 231 | Step 36000 | Mean PER: 0.1444343842484358 | Mean WER: 0.4611063829787234
Epoch: 231 | Step 36036 | Train Loss: 0.01351
Epoch: 232 | Step 36192 | Train Loss: 0.01241
Epoch: 233 | Step 36348 | Train Loss: 0.01316
Epoch: 234 | Step 36504 | Train Loss: 0.01269
Epoch: 235 | Step 36660 | Train Loss: 0.01176
Epoch: 236 | Step 36816 | Train Loss: 0.01255
Epoch: 237 | Step 36972 | Train Loss: 0.01265
Epoch: 238 | Step 37128 | Train Loss: 0.01108
Epoch: 239 | Step 37284 | Train Loss: 0.01436
Epoch: 240 | Step 37440 | Train Loss: 0.01278
Epoch: 241 | Step 37596 | Train Loss: 0.01243
Epoch: 242 | Step 37752 | Train Loss: 0.01121
Epoch: 243 | Step 37908 | Train Loss: 0.01163
Epoch: 244 | Step 38000 | Valid Loss: 0.01423
Epoch: 244 | Step 38000 | Mean PER: 0.14175867241449208 | Mean WER: 0.46502127659574466
Achieving better result (mean_per): 0.1418
Epoch: 244 | Step 38064 | Train Loss: 0.01347
Epoch: 245 | Step 38220 | Train Loss: 0.01073
Epoch: 246 | Step 38376 | Train Loss: 0.01047
Epoch: 247 | Step 38532 | Train Loss: 0.01239
Epoch: 248 | Step 38688 | Train Loss: 0.01130
Epoch: 249 | Step 38844 | Train Loss: 0.01178
Epoch: 250 | Step 39000 | Train Loss: 0.01269
Epoch: 251 | Step 39156 | Train Loss: 0.01126
Epoch: 252 | Step 39312 | Train Loss: 0.01104
Epoch: 253 | Step 39468 | Train Loss: 0.01104
Epoch: 254 | Step 39624 | Train Loss: 0.01139
Epoch: 255 | Step 39780 | Train Loss: 0.009937
Epoch: 256 | Step 39936 | Train Loss: 0.01118
Epoch: 257 | Step 40000 | Valid Loss: 0.01196
Epoch: 257 | Step 40000 | Mean PER: 0.14047487128204436 | Mean WER: 0.46093617021276595
Achieving better result (mean_per): 0.1405
Epoch: 257 | Step 40092 | Train Loss: 0.01065
Epoch: 258 | Step 40248 | Train Loss: 0.01128
Epoch: 259 | Step 40404 | Train Loss: 0.009863
Epoch: 260 | Step 40560 | Train Loss: 0.01099
Epoch: 261 | Step 40716 | Train Loss: 0.01157
Epoch: 262 | Step 40872 | Train Loss: 0.01108
Epoch: 263 | Step 41028 | Train Loss: 0.009422
Epoch: 264 | Step 41184 | Train Loss: 0.01042
Epoch: 265 | Step 41340 | Train Loss: 0.01133
Epoch: 266 | Step 41496 | Train Loss: 0.009604
Epoch: 267 | Step 41652 | Train Loss: 0.01061
Epoch: 268 | Step 41808 | Train Loss: 0.01024
Epoch: 269 | Step 41964 | Train Loss: 0.009998
Epoch: 270 | Step 42000 | Valid Loss: 0.008596
Epoch: 270 | Step 42000 | Mean PER: 0.1408262273814511 | Mean WER: 0.46076595744680854
Epoch: 270 | Step 42120 | Train Loss: 0.01013
Epoch: 271 | Step 42276 | Train Loss: 0.008533
Epoch: 272 | Step 42432 | Train Loss: 0.009475
Epoch: 273 | Step 42588 | Train Loss: 0.009258
Epoch: 274 | Step 42744 | Train Loss: 0.008571
Epoch: 275 | Step 42900 | Train Loss: 0.01009
Epoch: 276 | Step 43056 | Train Loss: 0.009416
Epoch: 277 | Step 43212 | Train Loss: 0.009066
Epoch: 278 | Step 43368 | Train Loss: 0.009770
Epoch: 279 | Step 43524 | Train Loss: 0.01002
Epoch: 280 | Step 43680 | Train Loss: 0.009680
Epoch: 281 | Step 43836 | Train Loss: 0.01071
Epoch: 282 | Step 43992 | Train Loss: 0.009669
Epoch: 283 | Step 44000 | Valid Loss: 0.02515
Epoch: 283 | Step 44000 | Mean PER: 0.14104244651954756 | Mean WER: 0.4636595744680851
Epoch: 283 | Step 44148 | Train Loss: 0.009646
Epoch: 284 | Step 44304 | Train Loss: 0.009802
Epoch: 285 | Step 44460 | Train Loss: 0.008851
Epoch: 286 | Step 44616 | Train Loss: 0.01044
Epoch: 287 | Step 44772 | Train Loss: 0.009096
Epoch: 288 | Step 44928 | Train Loss: 0.009339
Epoch: 289 | Step 45084 | Train Loss: 0.007797
Epoch: 290 | Step 45240 | Train Loss: 0.008977
Epoch: 291 | Step 45396 | Train Loss: 0.008603
Epoch: 292 | Step 45552 | Train Loss: 0.009981
Epoch: 293 | Step 45708 | Train Loss: 0.008109
Epoch: 294 | Step 45864 | Train Loss: 0.009121
Epoch: 295 | Step 46000 | Valid Loss: 0.008898
Epoch: 295 | Step 46000 | Mean PER: 0.14096136434276138 | Mean WER: 0.4645957446808511
Epoch: 295 | Step 46020 | Train Loss: 0.008926
Epoch: 296 | Step 46176 | Train Loss: 0.008755